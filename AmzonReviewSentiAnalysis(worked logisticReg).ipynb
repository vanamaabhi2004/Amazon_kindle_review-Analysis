{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIPMBMDh1G/v8AD+EUZve0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"es9ja3BzEMRz","executionInfo":{"status":"error","timestamp":1723047669994,"user_tz":-330,"elapsed":3586,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}},"outputId":"87ad3f8a-ee4c-4351-aa49-1a5948d7cba8"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/preprocessed_kindle_review .csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-be096357ba2a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Import the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/preprocessed_kindle_review .csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Display the first few rows of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/preprocessed_kindle_review .csv'"]}],"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from gensim.models import Word2Vec\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np\n","\n","# Import the dataset\n","\n","df = pd.read_csv('/content/preprocessed_kindle_review .csv')\n","\n","# Display the first few rows of the dataset\n","df.head()\n"]},{"cell_type":"code","source":["df.info()\n"],"metadata":{"id":"JWCu9E7kFAIS","executionInfo":{"status":"aborted","timestamp":1723047669997,"user_tz":-330,"elapsed":15,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"KoUjo7a2Fg0b","executionInfo":{"status":"aborted","timestamp":1723047669997,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.dropna(subset=['rating', 'reviewText'], inplace=True)"],"metadata":{"id":"Tt4VaLzQFlYj","executionInfo":{"status":"aborted","timestamp":1723047669998,"user_tz":-330,"elapsed":15,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"kZwaiWQGFsGq","executionInfo":{"status":"aborted","timestamp":1723047669998,"user_tz":-330,"elapsed":15,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df[['reviewText' , 'rating']]"],"metadata":{"id":"WtU0eXD4F1X4","executionInfo":{"status":"aborted","timestamp":1723047669998,"user_tz":-330,"elapsed":15,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"C31mXbPxGENU","executionInfo":{"status":"aborted","timestamp":1723047669998,"user_tz":-330,"elapsed":16,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')\n","\n","# Function to preprocess text\n","def preprocess_text(text):\n","    if isinstance(text, str):\n","        # Remove URLs\n","        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","        # Remove special characters and digits\n","        text = re.sub(r'\\[.*?\\]', '', text)\n","        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n","        text = text.lower()\n","        text = text.strip()\n","        # Tokenize text\n","        tokens = text.split()\n","        # Remove stopwords\n","        stop_words = set(stopwords.words('english'))\n","        tokens = [token for token in tokens if token not in stop_words]\n","        return tokens\n","    else:\n","        return []"],"metadata":{"id":"6rBFMuxjGHl_","executionInfo":{"status":"aborted","timestamp":1723047669999,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply preprocessing to the reviewText column and convert ratings to binary sentiment\n","df['cleaned_reviewText'] = df['reviewText'].apply(preprocess_text)\n","df['sentiment'] = df['rating'].apply(lambda x: 1 if x >= 3 else 0)"],"metadata":{"id":"uJz70nbSGQ0X","executionInfo":{"status":"aborted","timestamp":1723047669999,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop rows with empty cleaned_reviewText\n","df = df[df['cleaned_reviewText'].map(lambda d: len(d)) > 0]"],"metadata":{"id":"nrgBSoD3Gw7W","executionInfo":{"status":"aborted","timestamp":1723047670000,"user_tz":-330,"elapsed":15,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df[['cleaned_reviewText', 'sentiment']]"],"metadata":{"id":"gatJdETvHH-E","executionInfo":{"status":"aborted","timestamp":1723047670000,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"PcSq03OqG87T","executionInfo":{"status":"aborted","timestamp":1723047670000,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train Word2Vec model\n","w2v_model = Word2Vec(sentences=df['cleaned_reviewText'], vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Create a dictionary of word vectors\n","word_vectors = w2v_model.wv"],"metadata":{"id":"KVsrxZjRHL59","executionInfo":{"status":"aborted","timestamp":1723047670000,"user_tz":-330,"elapsed":13,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to compute the average word vectors for a review\n","def compute_average_word_vectors(tokens, model, num_features):\n","    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n","    num_words = 0.0\n","    index2word_set = set(model.index_to_key)\n","\n","    for token in tokens:\n","        if token in index2word_set:\n","            num_words += 1\n","            feature_vector = np.add(feature_vector, model[token])\n","\n","    if num_words > 0:\n","        feature_vector = np.divide(feature_vector, num_words)\n","    return feature_vector\n","\n","# Compute average word vectors for each review\n","num_features = 100\n","df['feature_vector'] = df['cleaned_reviewText'].apply(lambda tokens: compute_average_word_vectors(tokens, word_vectors, num_features))\n","\n","# Display the first few rows of the processed dataset with feature vectors\n","df.head()\n"],"metadata":{"id":"2dmbvpOeHSx0","executionInfo":{"status":"aborted","timestamp":1723047670001,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare feature matrix and labels\n","X = np.array(df['feature_vector'].tolist())\n","y = df['sentiment'].values\n","\n","# Split the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Display the shapes of the training and validation sets\n","print(f'Training set shape: {X_train.shape}')\n","print(f'Validation set shape: {X_val.shape}')\n"],"metadata":{"id":"JPrAAAISJIDn","executionInfo":{"status":"aborted","timestamp":1723047670001,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Logistic Regression model\n","model = LogisticRegression()\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Predict on the validation set\n","y_pred = model.predict(X_val)\n"],"metadata":{"id":"avXTezX0JTJe","executionInfo":{"status":"aborted","timestamp":1723047670001,"user_tz":-330,"elapsed":13,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model\n","accuracy = accuracy_score(y_val, y_pred)\n","report = classification_report(y_val, y_pred)\n","\n","print(f'Validation Accuracy: {accuracy:.4f}')\n","print('Classification Report:')\n","print(report)\n"],"metadata":{"id":"GnUEfz94JaKf","executionInfo":{"status":"aborted","timestamp":1723047670001,"user_tz":-330,"elapsed":13,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####this was added extraaa\n","\n","# Predict on the training set\n","y_train_pred = model.predict(X_train)\n","\n","# Calculate training accuracy\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","print(f'Training Accuracy: {train_accuracy:.4f}')\n","\n","# Predict on the validation set\n","y_val_pred = model.predict(X_val)\n","\n","# Calculate validation accuracy\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f'Validation Accuracy: {val_accuracy:.4f}')\n","\n","# Generate classification report for validation set\n","report = classification_report(y_val, y_val_pred)\n","print('Classification Report:')\n","print(report)\n"],"metadata":{"id":"LK7cB6KqK-3d","executionInfo":{"status":"aborted","timestamp":1723047670001,"user_tz":-330,"elapsed":13,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to predict sentiment of new reviews\n","def predict_sentiment(review, model, word_vectors, num_features):\n","    tokens = preprocess_text(review)\n","    feature_vector = compute_average_word_vectors(tokens, word_vectors, num_features)\n","    feature_vector = feature_vector.reshape(1, -1)\n","    prediction = model.predict(feature_vector)\n","    return 'Positive' if prediction[0] == 1 else 'Negative'\n","\n","# Example predictions\n","new_reviews = [\n","    \"I absolutely loved this book! The story was fantastic and the characters were well developed.\",\n","    \"This book was terrible. I couldn't even finish it. The plot was boring and predictable.\"\n","]\n","\n","for review in new_reviews:\n","    sentiment = predict_sentiment(review, model, word_vectors, num_features)\n","    print(f'Review: {review}\\nPredicted Sentiment: {sentiment}\\n')\n"],"metadata":{"id":"hisOwrOZJb-a","executionInfo":{"status":"aborted","timestamp":1723047670002,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LvPXZIgdK9O0","executionInfo":{"status":"aborted","timestamp":1723047670002,"user_tz":-330,"elapsed":14,"user":{"displayName":"abhiram vanama","userId":"16703902985783510790"}}},"execution_count":null,"outputs":[]}]}